{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a675a15-d2bf-4c5e-a141-f491b6dbba40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. What is anomaly detection and what is its purpose?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Anomaly detection is a technique used in data analysis to identify unusual or abnormal observations within a dataset. Its purpose is to distinguish these anomalous instances from the majority of normal data points. Anomalies can be defined as data points or patterns that significantly deviate from the expected or usual behavior. Anomaly detection is employed in various domains such as fraud detection, network intrusion detection, system health monitoring, and outlier detection in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70c625c5-6854-4406-9b73-1c1b6a88d550",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. What are the key challenges in anomaly detection?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#There are several key challenges in anomaly detection, including:\n",
    "\n",
    "#1 - Lack of labeled data: Anomaly detection often deals with unlabeled datasets, making it difficult to define anomalies precisely and train supervised models.\n",
    "\n",
    "#2 - Imbalanced datasets: Anomalies are typically rare compared to normal instances, leading to imbalanced class distributions that can affect the performance of traditional machine learning algorithms.\n",
    "\n",
    "#3 - Concept drift: The characteristics of anomalies and normal instances may change over time, requiring adaptive techniques to handle evolving data.\n",
    "\n",
    "#4 - Feature selection: Identifying the most relevant features for anomaly detection can be challenging, as anomalies may exhibit different patterns in different feature spaces.\n",
    "\n",
    "#5 - Scalability: Anomaly detection algorithms need to efficiently process large-scale datasets in real-time or near real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1f08bbc-4ed8-41bf-9f96-b321147e126b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Unsupervised anomaly detection and supervised anomaly detection differ in terms of the availability of labeled data during the training phase.\n",
    "\n",
    "#In unsupervised anomaly detection, the algorithm learns from unlabeled data, where anomalies are not explicitly identified or labeled during training. The algorithm aims to discover patterns that deviate significantly from the majority of the data points. Unsupervised methods include techniques such as statistical approaches (e.g., Gaussian distribution modeling), clustering-based methods, and density estimation.\n",
    "\n",
    "#In supervised anomaly detection, the algorithm is trained on a labeled dataset where anomalies are explicitly labeled as such. The algorithm learns to classify data points as normal or anomalous based on the labeled examples. Supervised methods involve using machine learning algorithms such as support vector machines (SVMs), decision trees, or neural networks, which are trained using the labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "832a32cd-74ba-489e-a601-d09c41db3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. What are the main categories of anomaly detection algorithms?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The main categories of anomaly detection algorithms include:\n",
    "\n",
    "#1 - Statistical Methods: These methods assume that normal data follows a specific statistical distribution (e.g., Gaussian) and detect anomalies based on deviations from this distribution.\n",
    "\n",
    "#2 - Machine Learning Methods: These methods use various machine learning techniques to train models that can distinguish anomalies from normal instances. Supervised learning algorithms use labeled data to classify anomalies, while unsupervised learning algorithms identify deviations from normal patterns.\n",
    "\n",
    "#3 - Proximity-based Methods: These methods measure the similarity or dissimilarity between data points and identify anomalies based on their distance or density compared to their neighbors.\n",
    "\n",
    "#4 - Information Theory Methods: These methods leverage concepts from information theory to measure the unexpectedness or entropy of data points, identifying anomalies as those with high information content.\n",
    "\n",
    "#5 - Spectral Methods: These methods analyze the spectral properties of data, such as eigenvalues and eigenvectors, to detect anomalies based on unusual spectral patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b31565f1-2dfe-4da0-84ff-77c6b81bbd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#Distance-based anomaly detection methods make the following main assumptions:\n",
    "\n",
    "#1 - Anomalies have significantly different distances to their neighbors compared to normal instances. They are considered as isolated or far away from the majority of the data points.\n",
    "\n",
    "#2 - Normal instances are densely packed and have similar distances to their neighbors. They form cohesive clusters or have consistent density patterns.\n",
    "\n",
    "#These assumptions form the basis for distance-based anomaly detection algorithms like k-nearest neighbors (KNN), local outlier factor (LOF), and density-based spatial clustering of applications with noise (DBSCAN). These methods measure distances or densities to identify instances that deviate from the expected patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5e849f-5606-421e-8bf1-bd87e2ba825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The Local Outlier Factor (LOF) algorithm computes anomaly scores by comparing the local density of a data point to the local densities of its neighbors. The steps involved in calculating the LOF anomaly score for a data point are as follows:\n",
    "\n",
    "#1 - Determine the k nearest neighbors of the data point based on a chosen distance metric.\n",
    "\n",
    "#2 - Compute the reachability distance of each neighbor, which represents the distance to reach the neighbor from the data point.\n",
    "\n",
    "#3 - Calculate the local reachability density (LRD) of the data point as the inverse of the average reachability distance of its k nearest neighbors.\n",
    "\n",
    "#4 - Compute the local outlier factor (LOF) of the data point as the average ratio of the LRD values of its neighbors to its own LRD.\n",
    "\n",
    "#A higher LOF value indicates that the data point has a lower density compared to its neighbors, suggesting it is an anomaly. LOF values greater than 1 indicate potential anomalies, while values significantly greater than 1 indicate stronger evidence of being anomalous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52327316-969d-439a-a1d3-0ebcf36db76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#The Isolation Forest algorithm has two key parameters:\n",
    "\n",
    "#1 - Number of trees (n_estimators): It determines the number of isolation trees to be created during the training process. Increasing the number of trees improves the algorithm's ability to detect anomalies but also increases the computational cost. A reasonable value for the number of trees is typically between 50 and 1,000.\n",
    "\n",
    "#2 - Subsample size (max_samples): It specifies the number of samples to be used for constructing each isolation tree. A smaller subsample size increases the randomness and diversity of the trees but can also reduce their effectiveness. A typical value for the subsample size is often set to the default value of 256 or chosen as a fraction of the total number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf672565-a800-4925-a17e-78b36b3d13f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. If a data point has only 2 neighbors of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#To compute the anomaly score using k-nearest neighbors (KNN), the anomaly score for a data point is determined based on the proportion of its K nearest neighbors that belong to a different class. In this case, if a data point has only 2 neighbors of the same class within a radius of 0.5 and K=10, the remaining 8 neighbors are assumed to be of a different class.\n",
    "\n",
    "#Therefore, the anomaly score would be calculated as the ratio of the number of different-class neighbors (8) to the total number of neighbors (K=10). In this case, the anomaly score would be 8/10 = 0.8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7bd0218-b17c-4be9-b5ed-d8ac835e628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "\n",
    "#Ans\n",
    "\n",
    "#In the Isolation Forest algorithm, the average path length is used to compute anomaly scores. The average path length represents the average number of edges traversed to isolate an instance in the forest.\n",
    "\n",
    "#With 100 trees, the expected average path length for a data point that is not an anomaly (in the absence of anomalies) would be approximately 2 * (log2(3000) - 1) = 18.55. This is derived from the formula for the average path length in isolation trees, which is 2 * (log2(n) - 1), where n is the number of instances in the dataset.\n",
    "\n",
    "#If a data point has an average path length of 5.0 compared to the average path length of the trees (18.55), its anomaly score would be calculated as (5.0 / 18.55). Therefore, the anomaly score for this data point would be approximately 0.27."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5137c7-48ee-4abc-acfa-8bed74de0266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
